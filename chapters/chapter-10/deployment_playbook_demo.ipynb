{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERNEL CHECK AND IMPORTS\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîß Kernel Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import gradio as gr\n",
    "    import streamlit\n",
    "    import flask\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    print(f\"   Gradio: {gr.__version__}\")\n",
    "    print(f\"   Streamlit: {streamlit.__version__}\")\n",
    "    print(f\"   Flask: {flask.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please run: pip install -r requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Real TinyLlama Model Loading\n",
    "\n",
    "We'll load TinyLlama once and use it across all applications. This ensures consistency and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: LOAD REAL TINYLLAMA MODEL\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"üöÄ STEP 1: LOAD REAL TINYLLAMA MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load TinyLlama model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ {model_name} loaded successfully!\")\n",
    "print(f\"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"   Parameters: 1.1B\")\n",
    "print(f\"   Provider: TinyLlama\")\n",
    "\n",
    "def chat_with_tinyllama(message, max_length=200):\n",
    "    \"\"\"Chat with TinyLlama model\"\"\"\n",
    "    prompt = f\"<|user|>\\n{message}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"<|assistant|>\" in full_response:\n",
    "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    \n",
    "    response = response.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nüß™ Testing model...\")\n",
    "test_response = chat_with_tinyllama(\"Hello! How are you?\")\n",
    "print(f\"ü§ñ TinyLlama: {test_response}\")\n",
    "print(\"\\n‚úÖ Model ready for all applications!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Launch Applications\n",
    "\n",
    "Now we'll create and launch each application in separate processes. Each will have its own URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: CREATE APPLICATION FILES\n",
    "print(\"üõ†Ô∏è STEP 2: CREATING APPLICATION FILES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create Gradio app file\n",
    "gradio_code = '''#!/usr/bin/env python3\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def chat_with_tinyllama(message, max_length=200):\n",
    "    prompt = f\"<|user|>\\\\n{message}\\\\n<|assistant|>\\\\n\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs, max_length=inputs.shape[1] + max_length,\n",
    "            num_return_sequences=1, temperature=0.7, do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1, no_repeat_ngram_size=3\n",
    "        )\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"<|assistant|>\" in full_response:\n",
    "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    return response.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").strip()\n",
    "\n",
    "def gradio_chat(message, history):\n",
    "    response = chat_with_tinyllama(message)\n",
    "    return response\n",
    "\n",
    "# Create interface\n",
    "with gr.Blocks(title=\"Real TinyLlama AI Assistant\") as demo:\n",
    "    gr.Markdown(\"# ü§ñ Real TinyLlama On-Device AI Assistant\")\n",
    "    gr.Markdown(\"**Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 | **Parameters**: 1.1B | **Provider**: TinyLlama\")\n",
    "    \n",
    "    with gr.Tab(\"üí¨ Chat\"):\n",
    "        chatbot = gr.Chatbot(label=\"Real AI Conversation\", height=400, type=\"messages\")\n",
    "        msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "        \n",
    "        def user(user_message, history):\n",
    "            return \"\", history + [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        def bot(history):\n",
    "            user_message = history[-1][\"content\"]\n",
    "            bot_message = chat_with_tinyllama(user_message)\n",
    "            history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "            return history\n",
    "        \n",
    "        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "'''\n",
    "\n",
    "with open('gradio_app.py', 'w') as f:\n",
    "    f.write(gradio_code)\n",
    "\n",
    "# Create Streamlit app file\n",
    "streamlit_code = '''#!/usr/bin/env python3\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "def chat_with_tinyllama(message, model, tokenizer, max_length=200):\n",
    "    prompt = f\"<|user|>\\\\n{message}\\\\n<|assistant|>\\\\n\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs, max_length=inputs.shape[1] + max_length,\n",
    "            num_return_sequences=1, temperature=0.7, do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1, no_repeat_ngram_size=3\n",
    "        )\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"<|assistant|>\" in full_response:\n",
    "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    return response.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").strip()\n",
    "\n",
    "st.set_page_config(page_title=\"Real TinyLlama AI Assistant\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
    "st.title(\"ü§ñ Real TinyLlama On-Device AI Assistant\")\n",
    "st.markdown(\"**Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 | **Parameters**: 1.1B | **Provider**: TinyLlama\")\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"What would you like to know?\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            response = chat_with_tinyllama(prompt, model, tokenizer)\n",
    "        st.markdown(response)\n",
    "    \n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "'''\n",
    "\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "# Create Flask API file\n",
    "flask_code = '''#!/usr/bin/env python3\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def chat_with_tinyllama(message, max_length=200):\n",
    "    prompt = f\"<|user|>\\\\n{message}\\\\n<|assistant|>\\\\n\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs, max_length=inputs.shape[1] + max_length,\n",
    "            num_return_sequences=1, temperature=0.7, do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1, no_repeat_ngram_size=3\n",
    "        )\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"<|assistant|>\" in full_response:\n",
    "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    return response.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").strip()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return jsonify({\n",
    "        \"message\": \"Real TinyLlama API\",\n",
    "        \"status\": \"online\",\n",
    "        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"endpoints\": [\"/chat - POST: Send messages to the AI\", \"/health - GET: Check API health\"]\n",
    "    })\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.get_json()\n",
    "    if not data or 'message' not in data:\n",
    "        return jsonify({\"error\": \"Message is required\"}), 400\n",
    "    \n",
    "    response = chat_with_tinyllama(data['message'])\n",
    "    return jsonify({\"message\": data['message'], \"response\": response, \"model\": \"TinyLlama\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5001, debug=True)\n",
    "'''\n",
    "\n",
    "with open('flask_api.py', 'w') as f:\n",
    "    f.write(flask_code)\n",
    "\n",
    "print(\"‚úÖ Application files created:\")\n",
    "print(\"   üìÑ gradio_app.py\")\n",
    "print(\"   üìÑ streamlit_app.py\")\n",
    "print(\"   üìÑ flask_api.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: LAUNCH GRADIO APPLICATION\n",
    "print(\"üåê STEP 3: LAUNCHING GRADIO APPLICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Kill any existing processes on port 7860 (clean way)\n",
    "try:\n",
    "    subprocess.run(['pkill', '-f', 'gradio'], check=False, capture_output=True)\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Launch Gradio in background\n",
    "gradio_process = subprocess.Popen(\n",
    "    ['python', 'gradio_app.py'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Gradio launching...\")\n",
    "time.sleep(5)  # Give it time to start\n",
    "\n",
    "print(\"\\n‚úÖ GRADIO APPLICATION READY!\")\n",
    "print(\"üåê URL: http://localhost:7860\")\n",
    "print(\"üí° Features: Real AI chat with TinyLlama\")\n",
    "print(\"üéØ Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(\"\\nüìù Note: Gradio is running in the background\")\n",
    "print(\"   Click the URL above to test the application\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: LAUNCH STREAMLIT APPLICATION\n",
    "print(\"üñ•Ô∏è STEP 4: LAUNCHING STREAMLIT APPLICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Kill any existing processes on port 8501 (clean way)\n",
    "try:\n",
    "    subprocess.run(['pkill', '-f', 'streamlit'], check=False, capture_output=True)\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Launch Streamlit in background\n",
    "streamlit_process = subprocess.Popen(\n",
    "    ['streamlit', 'run', 'streamlit_app.py', '--server.port=8501', '--server.headless=true'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Streamlit launching...\")\n",
    "time.sleep(5)  # Give it time to start\n",
    "\n",
    "print(\"\\n‚úÖ STREAMLIT APPLICATION READY!\")\n",
    "print(\"üåê URL: http://localhost:8501\")\n",
    "print(\"üí° Features: Real AI chat with TinyLlama\")\n",
    "print(\"üéØ Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(\"\\nüìù Note: Streamlit is running in the background\")\n",
    "print(\"   Click the URL above to test the application\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: LAUNCH FLASK API\n",
    "print(\"üîå STEP 5: LAUNCHING FLASK API\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Kill any existing processes on port 5001 (clean way)\n",
    "try:\n",
    "    subprocess.run(['pkill', '-f', 'flask'], check=False, capture_output=True)\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Launch Flask in background\n",
    "flask_process = subprocess.Popen(\n",
    "    ['python', 'flask_api.py'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Flask API launching...\")\n",
    "time.sleep(5)  # Give it time to start\n",
    "\n",
    "print(\"\\n‚úÖ FLASK API READY!\")\n",
    "print(\"üåê URL: http://localhost:5001\")\n",
    "print(\"üí° Features: REST API with real TinyLlama\")\n",
    "print(\"üéØ Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(\"\\nüìù Test with:\")\n",
    "print(\"   curl -X POST http://localhost:5001/chat \\\\\")\n",
    "print(\"        -H 'Content-Type: application/json' \\\\\")\n",
    "print(\"        -d '{\\\"message\\\": \\\"Hello!\\\"}'\")\n",
    "print(\"\\nüìù Note: Flask API is running in the background\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Deployment Complete!\n",
    "\n",
    "All three applications are now running with real TinyLlama AI:\n",
    "\n",
    "### üåê Application URLs:\n",
    "1. **Gradio Interface**: http://localhost:7860\n",
    "2. **Streamlit App**: http://localhost:8501  \n",
    "3. **Flask API**: http://localhost:5001\n",
    "\n",
    "### üß† Model Information:\n",
    "- **Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "- **Parameters**: 1.1B\n",
    "- **Provider**: TinyLlama\n",
    "- **Type**: Real AI (no mock models)\n",
    "\n",
    "### üéØ What You've Accomplished:\n",
    "- ‚úÖ Loaded a real TinyLlama model\n",
    "- ‚úÖ Created a Gradio web interface\n",
    "- ‚úÖ Built a Streamlit application\n",
    "- ‚úÖ Implemented a Flask REST API\n",
    "- ‚úÖ All apps use the same real AI model\n",
    "- ‚úÖ No mock models anywhere!\n",
    "\n",
    "### üß™ Test Your Applications:\n",
    "1. **Gradio**: Click http://localhost:7860 for interactive chat\n",
    "2. **Streamlit**: Click http://localhost:8501 for data analysis\n",
    "3. **Flask API**: Use curl or Postman to test http://localhost:5001/chat\n",
    "\n",
    "**All applications are running real AI - no simulations!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
