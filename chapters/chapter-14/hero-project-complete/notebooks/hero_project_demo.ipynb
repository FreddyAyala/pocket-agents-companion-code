{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Hero Project: On-Device AI Agent\n",
    "\n",
    "**This notebook loads and uses the real Qwen3-4B model!**\n",
    "\n",
    "This notebook demonstrates a complete local AI agent system using:\n",
    "- **Atomic Agents Framework** for agent orchestration\n",
    "- **Qwen3-4B-Instruct** for vision-language processing in GGUF format\n",
    "- **ChromaDB** for vector storage and RAG\n",
    "- **Real model loading** - downloads and uses the actual model!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. **Real Model Loading**: Download and load the actual Qwen3-4B model\n",
    "2. **RAG Q&A**: Answer questions using a knowledge base\n",
    "3. **Task Automation**: Execute file operations and system tasks\n",
    "4. **Agent Orchestration**: How to build composable AI agents\n",
    "5. **Local Deployment**: Complete on-device AI system\n",
    "\n",
    "## üöÄ Quick Setup\n",
    "\n",
    "This notebook will automatically download the model if needed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python Environment Check\n",
      "==================================================\n",
      "Python version: 3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "Python executable: /Users/freddyayala/Documents/GitHub/slm-ebook/companion-code/hero-project/venv/bin/python\n",
      "Current working directory: /Users/freddyayala/Documents/GitHub/slm-ebook/companion-code/hero-project/notebooks\n",
      "‚úÖ Changed to: /Users/freddyayala/Documents/GitHub/slm-ebook/companion-code/hero-project\n",
      "‚úÖ In correct directory\n",
      "‚úÖ Added src and current directory to Python path\n"
     ]
    }
   ],
   "source": [
    "# KERNEL CHECK - Make sure you're using the correct Python environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üêç Python Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Change to the correct directory\n",
    "# Note: Adjust this path based on your setup\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "hero_project_dir = os.path.join(notebook_dir, '..')\n",
    "os.chdir(hero_project_dir)\n",
    "print(f\"‚úÖ Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not os.path.exists('src/model_loader.py'):\n",
    "    print(\"‚ö†Ô∏è Warning: Not in hero-project directory. Please run from the hero-project-complete directory\")\n",
    "else:\n",
    "    print(\"‚úÖ In correct directory\")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "sys.path.append('.')\n",
    "print(\"‚úÖ Added src and current directory to Python path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "   PyTorch: 2.9.0\n",
      "   Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import our custom modules\n",
    "from src.model_loader import Qwen3VLLoader\n",
    "from src.vector_store import VectorStore\n",
    "from src.agents.rag_agent import RAGAgent\n",
    "from src.agents.task_agent import TaskAgent\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† STEP 1: LOAD THE REAL QWEN3-4B MODEL\n",
    "\n",
    "This will download and load the actual model (may take several minutes on first run).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading REAL Qwen3-4B-Instruct model...\n",
      "============================================================\n",
      "‚ö†Ô∏è This may take several minutes on first run (downloading ~2.5GB model)\n",
      "\n",
      "üì• Loading model (this may take a while)...\n",
      "üîÑ Loading unsloth/Qwen3-4B-Instruct-2507-GGUF in GGUF format...\n",
      "‚úÖ Using existing model: ./models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully on cpu\n",
      "   Model format: GGUF\n",
      "   Context window: 4096 tokens\n",
      "\n",
      "üìä Model Information:\n",
      "   model_path: unsloth/Qwen3-4B-Instruct-2507-GGUF\n",
      "   model_file: ./models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf\n",
      "   device: cpu\n",
      "   format: GGUF\n",
      "   context_window: 4096\n",
      "   status: loaded\n",
      "\n",
      "‚úÖ REAL Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and load the REAL model\n",
    "print(\"üîÑ Loading REAL Qwen3-4B-Instruct model...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è This may take several minutes on first run (downloading ~2.5GB model)\")\n",
    "\n",
    "model_loader = Qwen3VLLoader()\n",
    "\n",
    "# Actually load the model (this will download if needed)\n",
    "print(\"\\nüì• Loading model (this may take a while)...\")\n",
    "model = model_loader.load()\n",
    "\n",
    "# Get model information\n",
    "model_info = model_loader.get_model_info()\n",
    "print(\"\\nüìä Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ REAL Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing REAL text generation...\n",
      "==================================================\n",
      "ü§ñ REAL Model Response:\n",
      "Hello! üòä Absolutely ‚Äî let's dive into small language models!\n",
      "\n",
      "### What Are Small Language Models?\n",
      "\n",
      "Small language models (LLMs) are compact versions of large language models (LLMs) ‚Äî like GPT-3, LLaMA, or BERT ‚Äî that are designed to be more efficient in terms of **size, speed, and computational requirements**. Instead of having billions of parameters (like GPT-3 with 175 billion parameters), small language models typically have **tens to hundreds of millions of parameters**, making them much lighter and easier to run on devices like smartphones, laptops, or even edge hardware.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Characteristics of Small Language Models:\n",
      "\n",
      "1. **Smaller Size & Lower Resource Use**  \n",
      "   - They require significantly less RAM, GPU, or cloud computing power.\n",
      "   - Can be deployed locally on devices (e.g., your phone or laptop), enabling privacy and offline use.\n",
      "\n",
      "2. **Faster Inference**  \n",
      "   - Generate responses more quickly than large models.\n",
      "   - Ideal for real-time applications like chatbots, quick question answering, or voice assistants.\n",
      "\n",
      "3. **Lower Cost**  \n",
      "   - Training and deployment are cheaper, especially for startups or edge applications.\n",
      "\n",
      "4. **Simplified Fine-Tuning**  \n",
      "   - Easier to fine-tune on specific tasks (e.g., sentiment analysis, translation) without needing massive datasets.\n",
      "\n",
      "5. **Reduced Overfitting**  \n",
      "   - With fewer parameters, they may generalize better on smaller datasets and avoid memorizing training data.\n",
      "\n",
      "---\n",
      "\n",
      "### Examples of Small Language Models:\n",
      "\n",
      "- **TinyLlama** (130M parameters) ‚Äì A small, open-source model designed for efficiency and simplicity.\n",
      "- **Phi-1.5** (2.7B parameters) ‚Äì A model from Microsoft, optimized for small-scale but capable tasks.\n",
      "- **Llama-3-8B** (8 billion parameters) ‚Äì While not \"tiny,\" it's still smaller than the original Llama-2-70B, and can be fine-tuned or quantized further.\n",
      "- **Gemma-2B** (2 billion parameters) ‚Äì Google‚Äôs efficient model for mobile and edge devices.\n",
      "- **TinyBERT** ‚Äì A small version of the BERT model, optimized for tasks like text classification.\n",
      "\n",
      "---\n",
      "\n",
      "### Use Cases for Small Language Models:\n",
      "\n",
      "- **Mobile Apps** ‚Äì On-device chatbots or translation tools.\n",
      "- **IoT Devices** ‚Äì Smart speakers, wearables, or home assistants that don‚Äôt\n",
      "\n",
      "‚è±Ô∏è Generation time: 15.73 seconds\n",
      "üìù Response length: 2227 characters\n"
     ]
    }
   ],
   "source": [
    "# Test REAL text generation\n",
    "print(\"üß™ Testing REAL text generation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you tell me about small language models?\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = model_loader.generate_response(test_messages)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"ü§ñ REAL Model Response:\")\n",
    "print(f\"{response}\")\n",
    "print(f\"\\n‚è±Ô∏è Generation time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"üìù Response length: {len(response)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è STEP 2: SETUP VECTOR STORE WITH CHROMADB\n",
    "\n",
    "Create a knowledge base using ChromaDB for RAG (Retrieval-Augmented Generation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Setting up ChromaDB vector store...\n",
      "==================================================\n",
      "‚úÖ Vector store initialized: knowledge_base\n",
      "   Persist directory: ./chroma_db\n",
      "   Embedding model: all-MiniLM-L6-v2\n",
      "\n",
      "üìö Creating sample knowledge base...\n",
      "‚úÖ Created 4 sample documents in ./data/knowledge_base\n",
      "‚úÖ Added 4 documents to vector store\n",
      "‚úÖ Loaded 4 documents from ./data/knowledge_base\n",
      "\n",
      "üìä Vector Store Information:\n",
      "   collection_name: knowledge_base\n",
      "   document_count: 4\n",
      "   persist_directory: ./chroma_db\n",
      "   embedding_model: all-MiniLM-L6-v2\n",
      "\n",
      "‚úÖ Vector store ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "print(\"üîÑ Setting up ChromaDB vector store...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Create sample documents\n",
    "print(\"\\nüìö Creating sample knowledge base...\")\n",
    "vector_store.save_sample_documents()\n",
    "\n",
    "# Get collection info\n",
    "collection_info = vector_store.get_collection_info()\n",
    "print(\"\\nüìä Vector Store Information:\")\n",
    "for key, value in collection_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Vector store ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing vector search...\n",
      "========================================\n",
      "Query: What are small language models?\n",
      "Found 2 relevant documents:\n",
      "\n",
      "üìÑ Document 1:\n",
      "   Source: small_language_models.txt\n",
      "   Content: Small Language Models (SLMs) are compact versions of large language models designed \n",
      "            to run efficiently on local devices. They typically have fewer than 10 billion parameters \n",
      "            ...\n",
      "\n",
      "üìÑ Document 2:\n",
      "   Source: vision_language_models.txt\n",
      "   Content: Vision-Language Models (VLMs) are AI models that can process and understand both \n",
      "            visual and textual information. They can analyze images, answer questions about visual \n",
      "            conten...\n",
      "\n",
      "‚úÖ Vector search working!\n"
     ]
    }
   ],
   "source": [
    "# Test vector search\n",
    "print(\"üîç Testing vector search...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_query = \"What are small language models?\"\n",
    "search_results = vector_store.search(test_query, n_results=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Found {search_results['count']} relevant documents:\")\n",
    "print()\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(search_results['documents'], search_results['metadatas']), 1):\n",
    "    print(f\"üìÑ Document {i}:\")\n",
    "    print(f\"   Source: {metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"   Content: {doc[:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Vector search working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ STEP 3: TEST RAG AGENT WITH REAL MODEL\n",
    "\n",
    "Test the RAG agent with the actual loaded model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing RAG Agent with REAL model...\n",
      "==================================================\n",
      "‚úÖ RAG Agent initialized\n",
      "‚úÖ RAG Agent initialized with REAL model!\n",
      "\n",
      "üß™ Testing RAG Agent with REAL responses...\n",
      "üîç Searching knowledge base for: 'What is artificial intelligence?'\n",
      "ü§ñ Generating response...\n",
      "\n",
      "ü§ñ Question: What is artificial intelligence?\n",
      "ü§ñ REAL Answer: Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding.\n",
      "üìö Context used: 1201 characters\n",
      "\n",
      "‚úÖ RAG Agent working with REAL model!\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG agent with REAL model\n",
    "print(\"üîÑ Initializing RAG Agent with REAL model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rag_agent = RAGAgent(model_loader, vector_store)\n",
    "print(\"‚úÖ RAG Agent initialized with REAL model!\")\n",
    "\n",
    "# Test RAG agent with a question\n",
    "print(\"\\nüß™ Testing RAG Agent with REAL responses...\")\n",
    "test_question = \"What is artificial intelligence?\"\n",
    "result = rag_agent.run(test_question)\n",
    "\n",
    "print(f\"\\nü§ñ Question: {test_question}\")\n",
    "print(f\"ü§ñ REAL Answer: {result['answer']}\")\n",
    "print(f\"üìö Context used: {len(result['context'])} characters\")\n",
    "\n",
    "print(\"\\n‚úÖ RAG Agent working with REAL model!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è STEP 4: TEST TASK AGENT WITH REAL MODEL\n",
    "\n",
    "Test the Task agent with the actual loaded model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing Task Agent with REAL model...\n",
      "==================================================\n",
      "‚úÖ Task Agent initialized\n",
      "‚úÖ Task Agent initialized with REAL model!\n",
      "\n",
      "üß™ Testing Task Agent with REAL responses...\n",
      "ü§ñ Processing task: 'Create a file called test_agent.txt with the content: Hello from the REAL AI agent!'\n",
      "üîß Agent decided to use tool: file_write\n",
      "\n",
      "ü§ñ Task: Create a file called test_agent.txt with the content: Hello from the REAL AI agent!\n",
      "ü§ñ REAL Result: The file `test_agent.txt` has been successfully created with the content: \"Hello from the REAL AI agent!\".\n",
      "üîß Tools used: ['file_write']\n",
      "\n",
      "‚úÖ Task Agent working with REAL model!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Task Agent with REAL model\n",
    "print(\"üîÑ Initializing Task Agent with REAL model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "task_agent = TaskAgent(model_loader)\n",
    "print(\"‚úÖ Task Agent initialized with REAL model!\")\n",
    "\n",
    "# Test Task agent with file operations\n",
    "print(\"\\nüß™ Testing Task Agent with REAL responses...\")\n",
    "test_task = \"Create a file called test_agent.txt with the content: Hello from the REAL AI agent!\"\n",
    "result = task_agent.run(test_task)\n",
    "\n",
    "print(f\"\\nü§ñ Task: {test_task}\")\n",
    "print(f\"ü§ñ REAL Result: {result['result']}\")\n",
    "print(f\"üîß Tools used: {result['tools_used']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Task Agent working with REAL model!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ FINAL DEMONSTRATION WITH REAL MODEL\n",
    "\n",
    "Complete demonstration using the actual Qwen3-4B model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ HERO PROJECT: COMPLETE DEMONSTRATION WITH REAL MODEL\n",
      "======================================================================\n",
      "\n",
      "üîç DEMO 1: RAG Agent with REAL Qwen3-4B\n",
      "--------------------------------------------------\n",
      "üîç Searching knowledge base for: 'What are the benefits of small language models?'\n",
      "ü§ñ Generating response...\n",
      "Question: What are the benefits of small language models?\n",
      "REAL Answer: The benefits of small language models (SLMs) include:\n",
      "\n",
      "- **Local processing and privacy**: Models can run on local devices, reducing the need to send data to remote servers and enhancing user privacy.  \n",
      "- **Lower computational requirements**: SLMs require less power and hardware resources, making th...\n",
      "\n",
      "üîß DEMO 2: Task Agent with REAL Qwen3-4B\n",
      "--------------------------------------------------\n",
      "ü§ñ Processing task: 'Create a file called hero_demo_real.txt with a summary of what we learned about AI'\n",
      "üîß Agent decided to use tool: file_write\n",
      "Task: Create a file called hero_demo_real.txt with a summary of what we learned about AI\n",
      "REAL Result: The file `hero_demo_real.txt` has been successfully created with a summary of what we learned about AI. The final answer is that the summary has been saved as requested.\n",
      "\n",
      "üìÅ DEMO 3: Verify File Creation\n",
      "--------------------------------------------------\n",
      "‚úÖ File created successfully!\n",
      "Content: Summary of What We Learned About AI:\\n\\n1. AI systems are designed to simulate human intelligence in tasks such as learning...\n",
      "\n",
      "üéâ HERO PROJECT DEMONSTRATION COMPLETE WITH REAL MODEL!\n",
      "‚úÖ RAG Agent: Working with REAL Qwen3-4B\n",
      "‚úÖ Task Agent: Working with REAL Qwen3-4B\n",
      "‚úÖ File Operations: Working\n",
      "‚úÖ Real Agentic AI: ACHIEVED WITH REAL MODEL!\n",
      "\n",
      "üöÄ This is a fully functional, local AI agent system!\n"
     ]
    }
   ],
   "source": [
    "# üéâ FINAL DEMONSTRATION WITH REAL MODEL\n",
    "print(\"üéØ HERO PROJECT: COMPLETE DEMONSTRATION WITH REAL MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: RAG Agent with REAL model\n",
    "print(\"\\nüîç DEMO 1: RAG Agent with REAL Qwen3-4B\")\n",
    "print(\"-\" * 50)\n",
    "rag_question = \"What are the benefits of small language models?\"\n",
    "rag_result = rag_agent.run(rag_question)\n",
    "print(f\"Question: {rag_question}\")\n",
    "print(f\"REAL Answer: {rag_result['answer'][:300]}...\")\n",
    "\n",
    "# Test 2: Task Agent with REAL model\n",
    "print(\"\\nüîß DEMO 2: Task Agent with REAL Qwen3-4B\")\n",
    "print(\"-\" * 50)\n",
    "task_instruction = \"Create a file called hero_demo_real.txt with a summary of what we learned about AI\"\n",
    "task_result = task_agent.run(task_instruction)\n",
    "print(f\"Task: {task_instruction}\")\n",
    "print(f\"REAL Result: {task_result['result']}\")\n",
    "\n",
    "# Test 3: Verify file creation\n",
    "print(\"\\nüìÅ DEMO 3: Verify File Creation\")\n",
    "print(\"-\" * 50)\n",
    "try:\n",
    "    with open('hero_demo_real.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "    print(f\"‚úÖ File created successfully!\")\n",
    "    print(f\"Content: {content[:200]}...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found\")\n",
    "\n",
    "print(\"\\nüéâ HERO PROJECT DEMONSTRATION COMPLETE WITH REAL MODEL!\")\n",
    "print(\"‚úÖ RAG Agent: Working with REAL Qwen3-4B\")\n",
    "print(\"‚úÖ Task Agent: Working with REAL Qwen3-4B\") \n",
    "print(\"‚úÖ File Operations: Working\")\n",
    "print(\"‚úÖ Real Agentic AI: ACHIEVED WITH REAL MODEL!\")\n",
    "print(\"\\nüöÄ This is a fully functional, local AI agent system!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hero Project (venv)",
   "language": "python",
   "name": "hero-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
