{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Hero Project: On-Device AI Agent with Vision and RAG\n",
    "\n",
    "**Pocket Agents: A Practical Guide to Onâ€‘Device Artificial Intelligence**\n",
    "\n",
    "This notebook demonstrates a complete local AI agent system using:\n",
    "- **Atomic Agents Framework** for agent orchestration\n",
    "- **Qwen3-4B-Instruct** for vision-language processing in GGUF format\n",
    "- **ChromaDB** for vector storage and RAG\n",
    "- **Local processing** - everything runs on your device!\n",
    "\n",
    "## ğŸ¯ What You'll Learn\n",
    "\n",
    "1. **RAG Q&A**: Answer questions using a knowledge base with optional image context\n",
    "2. **Vision Capabilities**: Process uploaded images for analysis\n",
    "3. **Task Automation**: Execute file operations and system tasks\n",
    "4. **Agent Orchestration**: How to build composable AI agents\n",
    "5. **Local Deployment**: Complete on-device AI system\n",
    "\n",
    "## ğŸš€ Quick Setup\n",
    "\n",
    "Run the setup script: `./setup_and_test.sh`\n",
    "\n",
    "## ğŸ“‹ Learning Flow\n",
    "\n",
    "1. **Model Loading**: Load Qwen3-4B-Instruct with vision support\n",
    "2. **Vector Store**: Set up ChromaDB with sample knowledge base\n",
    "3. **RAG Agent**: Test question answering with retrieval\n",
    "4. **Task Agent**: Test task automation with tools\n",
    "5. **Vision Demo**: Test image analysis capabilities\n",
    "6. **Performance**: Measure system performance\n",
    "7. **Gradio UI**: Launch interactive web interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ Python Environment Check\n",
      "==================================================\n",
      "Python version: 3.11.9 (main, May 12 2025, 23:53:03) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "Python executable: /Users/freddyayala/Documents/GitHub/slm-ebook/companion-code/chapters/chapter-08/venv/bin/python3\n",
      "Current working directory: /Users/freddyayala/Documents/GitHub/slm-ebook/companion-code/hero-project/notebooks\n",
      "âš ï¸ Warning: Not in hero-project directory. Please run from companion-code/hero-project/\n",
      "âœ… Added src to Python path\n"
     ]
    }
   ],
   "source": [
    "# KERNEL CHECK - Make sure you're using the correct Python environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸ Python Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Change to the correct directory\n",
    "# Note: Adjust this path based on your setup\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "hero_project_dir = os.path.join(notebook_dir, '..')\n",
    "os.chdir(hero_project_dir)\n",
    "print(f\"âœ… Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not os.path.exists('src/model_loader.py'):\n",
    "    print(\"âš ï¸ Warning: Not in hero-project directory. Please run from companion-code/hero-project/\")\n",
    "else:\n",
    "    print(\"âœ… In correct directory\")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "sys.path.append('.')\n",
    "print(\"âœ… Added src and current directory to Python path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Import our custom modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen3VLLoader\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvector_store\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStore\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrag_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGAgent\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'model_loader'"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import our custom modules\n",
    "from src.model_loader import Qwen3VLLoader\n",
    "from src.vector_store import VectorStore\n",
    "from src.agents.rag_agent import RAGAgent\n",
    "from src.agents.task_agent import TaskAgent\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  STEP 1: LOAD QWEN3-4B-INSTRUCT MODEL\n",
    "\n",
    "Load the Qwen3-4B-Instruct model for text generation and tool calling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load the model\n",
    "print(\"ğŸ”„ Loading Qwen3-4B-Instruct model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_loader = Qwen3VLLoader()\n",
    "\n",
    "# Get model information\n",
    "model_info = model_loader.get_model_info()\n",
    "print(\"\\nğŸ“Š Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ… Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic text generation\n",
    "print(\"ğŸ§ª Testing basic text generation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you tell me about small language models?\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = model_loader.generate_response(test_messages)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"ğŸ¤– Model Response:\")\n",
    "print(f\"{response}\")\n",
    "print(f\"\\nâ±ï¸ Generation time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"ğŸ“ Response length: {len(response)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ STEP 2: SETUP VECTOR STORE WITH CHROMADB\n",
    "\n",
    "Create a knowledge base using ChromaDB for RAG (Retrieval-Augmented Generation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "print(\"ğŸ”„ Setting up ChromaDB vector store...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Create sample documents\n",
    "print(\"\\nğŸ“š Creating sample knowledge base...\")\n",
    "vector_store.save_sample_documents()\n",
    "\n",
    "# Get collection info\n",
    "collection_info = vector_store.get_collection_info()\n",
    "print(\"\\nğŸ“Š Vector Store Information:\")\n",
    "for key, value in collection_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ… Vector store ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector search\n",
    "print(\"ğŸ” Testing vector search...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_query = \"What are small language models?\"\n",
    "search_results = vector_store.search(test_query, n_results=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Found {search_results['count']} relevant documents:\")\n",
    "print()\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(search_results['documents'], search_results['metadatas']), 1):\n",
    "    print(f\"ğŸ“„ Document {i}:\")\n",
    "    print(f\"   Source: {metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"   Content: {doc[:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Vector search working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– STEP 3: INITIALIZE RAG AGENT\n",
    "\n",
    "Create the RAG agent that combines retrieval with generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Initializing RAG Agent...\n",
      "========================================\n",
      "âœ… RAG Agent initialized\n",
      "âœ… RAG Agent initialized!\n",
      "\n",
      "ğŸ§ª Testing RAG Agent...\n",
      "ğŸ” Searching knowledge base for: 'What is artificial intelligence?'\n",
      "ğŸ¤– Generating response...\n",
      "\n",
      "ğŸ¤– Question: What is artificial intelligence?\n",
      "ğŸ¤– Answer: Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding.\n",
      "ğŸ“š Context used: 1201 characters\n",
      "\n",
      "âœ… RAG Agent working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG agent\n",
    "print(\"ğŸ”„ Initializing RAG Agent...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "rag_agent = RAGAgent(model_loader, vector_store)\n",
    "print(\"âœ… RAG Agent initialized!\")\n",
    "\n",
    "# Test RAG agent with a question\n",
    "print(\"\\nğŸ§ª Testing RAG Agent...\")\n",
    "test_question = \"What is artificial intelligence?\"\n",
    "result = rag_agent.run(test_question)\n",
    "\n",
    "print(f\"\\nğŸ¤– Question: {test_question}\")\n",
    "print(f\"ğŸ¤– Answer: {result['answer']}\")\n",
    "print(f\"ğŸ“š Context used: {len(result['context'])} characters\")\n",
    "\n",
    "print(\"\\nâœ… RAG Agent working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Initializing Task Agent...\n",
      "========================================\n",
      "âœ… Task Agent initialized\n",
      "âœ… Task Agent initialized!\n",
      "\n",
      "ğŸ§ª Testing Task Agent...\n",
      "ğŸ¤– Processing task: 'Create a file called test_agent.txt with the content: Hello from the AI agent!'\n",
      "ğŸ”§ Agent decided to use tool: file_write\n",
      "\n",
      "ğŸ¤– Task: Create a file called test_agent.txt with the content: Hello from the AI agent!\n",
      "ğŸ¤– Result: The file `test_agent.txt` has been successfully created with the content: \"Hello from the AI agent!\"\n",
      "ğŸ”§ Tools used: ['file_write']\n",
      "\n",
      "âœ… Task Agent working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Task Agent\n",
    "print(\"ğŸ”„ Initializing Task Agent...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "task_agent = TaskAgent(model_loader)\n",
    "print(\"âœ… Task Agent initialized!\")\n",
    "\n",
    "# Test Task agent with file operations\n",
    "print(\"\\nğŸ§ª Testing Task Agent...\")\n",
    "test_task = \"Create a file called test_agent.txt with the content: Hello from the AI agent!\"\n",
    "result = task_agent.run(test_task)\n",
    "\n",
    "print(f\"\\nğŸ¤– Task: {test_task}\")\n",
    "print(f\"ğŸ¤– Result: {result['result']}\")\n",
    "print(f\"ğŸ”§ Tools used: {result['tools_used']}\")\n",
    "\n",
    "print(\"\\nâœ… Task Agent working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ HERO PROJECT: COMPLETE DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "ğŸ” DEMO 1: RAG Agent Question Answering\n",
      "--------------------------------------------------\n",
      "ğŸ” Searching knowledge base for: 'What are the benefits of small language models?'\n",
      "ğŸ¤– Generating response...\n",
      "Question: What are the benefits of small language models?\n",
      "Answer: The benefits of small language models (SLMs) include:\n",
      "\n",
      "- **Local processing and privacy**: Models can run on local devices, reducing the need to send data to remote servers and enhancing user privacy....\n",
      "\n",
      "ğŸ”§ DEMO 2: Task Agent File Operations\n",
      "--------------------------------------------------\n",
      "ğŸ¤– Processing task: 'Create a file called hero_demo.txt with a summary of what we learned'\n",
      "ğŸ”§ Agent decided to use tool: file_write\n",
      "Task: Create a file called hero_demo.txt with a summary of what we learned\n",
      "Result: The file `hero_demo.txt` has been successfully created with a summary of what we learned. The summary includes key aspects of autonomous decision-making, tool usage, proactive behavior, information gathering, file handling, and multi-step task execution. This demonstrates the agent's ability to function independently and efficiently.\n",
      "\n",
      "ğŸ“ DEMO 3: Verify File Creation\n",
      "--------------------------------------------------\n",
      "âœ… File created successfully!\n",
      "Content: Summary of What We Learned:\\n\\n- Autonomous decision-making: The agent can independently decide whic...\n",
      "\n",
      "ğŸ‰ HERO PROJECT DEMONSTRATION COMPLETE!\n",
      "âœ… RAG Agent: Working\n",
      "âœ… Task Agent: Working\n",
      "âœ… File Operations: Working\n",
      "âœ… Real Agentic AI: Achieved!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ‰ FINAL DEMONSTRATION\n",
    "print(\"ğŸ¯ HERO PROJECT: COMPLETE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: RAG Agent\n",
    "print(\"\\nğŸ” DEMO 1: RAG Agent Question Answering\")\n",
    "print(\"-\" * 50)\n",
    "rag_question = \"What are the benefits of small language models?\"\n",
    "rag_result = rag_agent.run(rag_question)\n",
    "print(f\"Question: {rag_question}\")\n",
    "print(f\"Answer: {rag_result['answer'][:200]}...\")\n",
    "\n",
    "# Test 2: Task Agent\n",
    "print(\"\\nğŸ”§ DEMO 2: Task Agent File Operations\")\n",
    "print(\"-\" * 50)\n",
    "task_instruction = \"Create a file called hero_demo.txt with a summary of what we learned\"\n",
    "task_result = task_agent.run(task_instruction)\n",
    "print(f\"Task: {task_instruction}\")\n",
    "print(f\"Result: {task_result['result']}\")\n",
    "\n",
    "# Test 3: Verify file creation\n",
    "print(\"\\nğŸ“ DEMO 3: Verify File Creation\")\n",
    "print(\"-\" * 50)\n",
    "try:\n",
    "    with open('hero_demo.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "    print(f\"âœ… File created successfully!\")\n",
    "    print(f\"Content: {content[:100]}...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ File not found\")\n",
    "\n",
    "print(\"\\nğŸ‰ HERO PROJECT DEMONSTRATION COMPLETE!\")\n",
    "print(\"âœ… RAG Agent: Working\")\n",
    "print(\"âœ… Task Agent: Working\") \n",
    "print(\"âœ… File Operations: Working\")\n",
    "print(\"âœ… Real Agentic AI: Achieved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
