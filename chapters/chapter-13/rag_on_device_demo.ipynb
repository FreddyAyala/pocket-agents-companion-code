{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Retrieval-Augmented Generation (RAG) On-Device\n",
    "\n",
    "This notebook demonstrates how to build **Retrieval-Augmented Generation (RAG)** systems that run entirely on-device, combining local documents with local intelligence for private, efficient AI applications.\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand why local grounding matters for privacy and performance\n",
    "- Implement lightweight vector databases for edge deployment\n",
    "- Build complete on-device RAG pipelines with local chunking and embeddings\n",
    "- Optimize search and retrieval with hybrid ranking algorithms\n",
    "- Compare different vector database solutions (ChromaDB, Faiss, SQLite)\n",
    "- Analyze trade-offs between speed, simplicity, and memory footprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Local Grounding Matters\n",
    "\n",
    "**Local RAG** combines the power of retrieval-augmented generation with complete data sovereignty. Unlike cloud-based RAG systems, on-device RAG ensures:\n",
    "\n",
    "- **Complete Privacy**: Your documents never leave your device\n",
    "- **Zero Latency**: No network calls for document retrieval\n",
    "- **Offline Capability**: Works without internet connection\n",
    "- **Cost Efficiency**: No API costs for embedding or retrieval\n",
    "- **Data Sovereignty**: You control your knowledge base entirely\n",
    "\n",
    "This is especially critical for sensitive documents, proprietary information, or when working in environments with strict data governance requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Databases for the Edge\n",
    "\n",
    "For on-device RAG, we need lightweight vector databases that can run efficiently on local hardware. The main options are:\n",
    "\n",
    "### **ChromaDB**: Excellent for Prototyping\n",
    "- **Pros**: Simple API, good documentation, built-in persistence\n",
    "- **Cons**: Higher memory overhead, slower for large datasets\n",
    "- **Best for**: Development, small-scale deployments, learning\n",
    "\n",
    "### **Faiss**: Pure Indexing Speed\n",
    "- **Pros**: Extremely fast, optimized for production, low memory usage\n",
    "- **Cons**: More complex setup, requires more code\n",
    "- **Best for**: Production applications, large document collections\n",
    "\n",
    "### **SQLite/DuckDB Extensions**: Minimal Footprint\n",
    "- **Pros**: Smallest footprint, embedded database, familiar SQL interface\n",
    "- **Cons**: Limited vector operations, requires extensions\n",
    "- **Best for**: Embedded systems, resource-constrained environments\n",
    "\n",
    "### **Trade-off Analysis**\n",
    "- **Speed**: Faiss > ChromaDB > SQLite\n",
    "- **Simplicity**: ChromaDB > SQLite > Faiss  \n",
    "- **Memory**: SQLite < Faiss < ChromaDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The On-Device RAG Pipeline\n",
    "\n",
    "A complete on-device RAG system consists of several key components:\n",
    "\n",
    "1. **Document Chunking**: Split documents into manageable pieces\n",
    "2. **Embedding Generation**: Convert text chunks to vector representations\n",
    "3. **Vector Storage**: Store embeddings in a local vector database\n",
    "4. **Query Processing**: Convert user queries to embeddings\n",
    "5. **Nearest Neighbor Search**: Find most relevant document chunks\n",
    "6. **Result Ranking**: Score and rank retrieved results\n",
    "7. **Context Construction**: Build prompts with retrieved context\n",
    "\n",
    "This pipeline runs entirely on your device, ensuring complete privacy and control over your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Environment configured for lightweight operation\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup for On-Device RAG\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure environment for stable on-device operation\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "print(\"Environment configured for on-device RAG operations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries for On-Device RAG\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"On-device RAG libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading embedding model...\n",
      "✅ Model loaded successfully\n",
      "📊 Model dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load On-Device Embedding Model\n",
    "print(\"Loading lightweight embedding model for on-device use...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"On-device embedding model loaded successfully\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(\"This model runs entirely on your device - no cloud dependencies!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Test embedding shape: (1, 384)\n",
      "📝 Test embedding sample: [ 0.00697795  0.11693418  0.08613165  0.03641429 -0.06965613]...\n"
     ]
    }
   ],
   "source": [
    "# Test Basic Embedding Generation\n",
    "test_text = \"This is a test sentence for RAG.\"\n",
    "embedding = model.encode([test_text])\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Sample values: {embedding[0][:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Loaded 5 sample documents\n",
      "  1. Machine learning is transforming healthcare by enabling earl...\n",
      "  2. Artificial intelligence and robotics are advancing rapidly, ...\n",
      "  3. Natural language processing helps computers understand and g...\n",
      "  4. Computer vision systems can analyze images and videos to ide...\n",
      "  5. Deep learning neural networks mimic the human brain to solve...\n"
     ]
    }
   ],
   "source": [
    "# Local Knowledge Base\n",
    "documents = [\n",
    "    \"Machine learning is transforming healthcare by enabling early disease detection and personalized treatment plans.\",\n",
    "    \"Artificial intelligence and robotics are advancing rapidly, creating new opportunities in manufacturing and automation.\",\n",
    "    \"Natural language processing helps computers understand and generate human language for better communication.\",\n",
    "    \"Computer vision systems can analyze images and videos to identify objects, faces, and patterns.\",\n",
    "    \"Deep learning neural networks mimic the human brain to solve complex problems in various domains.\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} local documents for on-device RAG system\")\n",
    "print(\"These documents are stored locally and never sent to external services\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"  {i+1}. {doc[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating embeddings...\n",
      "✅ Generated embeddings: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "# Generate Document Embeddings\n",
    "print(\"Generating embeddings for all documents...\")\n",
    "document_embeddings = model.encode(documents)\n",
    "print(f\"Generated embeddings: {document_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Query: How does machine learning help in healthcare?\n",
      "\n",
      "📊 Similarity scores:\n",
      "  1. 0.664 - Machine learning is transforming healthcare by ena...\n",
      "  2. 0.247 - Artificial intelligence and robotics are advancing...\n",
      "  3. 0.323 - Natural language processing helps computers unders...\n",
      "  4. 0.314 - Computer vision systems can analyze images and vid...\n",
      "  5. 0.379 - Deep learning neural networks mimic the human brai...\n"
     ]
    }
   ],
   "source": [
    "# Test Similarity Search\n",
    "query = \"How does machine learning help in healthcare?\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = cosine_similarity(query_embedding, document_embeddings)[0]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\nSimilarity scores:\")\n",
    "for i, (doc, sim) in enumerate(zip(documents, similarities)):\n",
    "    print(f\"  {i+1}. {sim:.3f} - {doc[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Simple RAG function defined\n"
     ]
    }
   ],
   "source": [
    "# Basic RAG Implementation\n",
    "def simple_rag(query, documents, model, top_k=2):\n",
    "    \"\"\"Basic RAG implementation for document retrieval\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Generate document embeddings\n",
    "    doc_embeddings = model.encode(documents)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'document': documents[idx],\n",
    "            'similarity': similarities[idx],\n",
    "            'rank': len(results) + 1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"RAG function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query 1: How does machine learning help in healthcare?\n",
      "  📄 Rank 1: 0.664 - Machine learning is transforming healthcare by enabling earl...\n",
      "  📄 Rank 2: 0.379 - Deep learning neural networks mimic the human brain to solve...\n",
      "\n",
      "🔍 Query 2: What are the applications of computer vision?\n",
      "  📄 Rank 1: 0.662 - Computer vision systems can analyze images and videos to ide...\n",
      "  📄 Rank 2: 0.346 - Natural language processing helps computers understand and g...\n",
      "\n",
      "🔍 Query 3: How do neural networks work?\n",
      "  📄 Rank 1: 0.506 - Deep learning neural networks mimic the human brain to solve...\n",
      "  📄 Rank 2: 0.322 - Computer vision systems can analyze images and videos to ide...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG with Multiple Queries\n",
    "queries = [\n",
    "    \"How does machine learning help in healthcare?\",\n",
    "    \"What are the applications of computer vision?\",\n",
    "    \"How do neural networks work?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\nQuery {i+1}: {query}\")\n",
    "    results = simple_rag(query, documents, model, top_k=2)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"  Rank {result['rank']}: {result['similarity']:.3f} - {result['document'][:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️ Testing ChromaDB...\n",
      "✅ ChromaDB search successful\n",
      "📊 Found 3 results\n",
      "  1. Machine learning is transforming healthcare by ena...\n",
      "  2. Computer vision systems can analyze images and vid...\n",
      "  3. Natural language processing helps computers unders...\n"
     ]
    }
   ],
   "source": [
    "# Vector Database Comparison: ChromaDB vs Alternatives\n",
    "try:\n",
    "    import chromadb\n",
    "    \n",
    "    print(\"Testing ChromaDB for on-device RAG...\")\n",
    "    print(\"ChromaDB: Excellent for prototyping and small-scale deployments\")\n",
    "    \n",
    "    client = chromadb.Client()\n",
    "    collection = client.create_collection(\"local_rag_docs\")\n",
    "    \n",
    "    # Add documents to local vector store\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "    )\n",
    "    \n",
    "    # Test local search\n",
    "    results = collection.query(\n",
    "        query_texts=[\"What is machine learning?\"],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    print(\"ChromaDB on-device search successful\")\n",
    "    print(f\"Found {len(results['documents'][0])} results locally\")\n",
    "    \n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        print(f\"  {i+1}. {doc[:50]}...\")\n",
    "        \n",
    "    print(\"\\nChromaDB Trade-offs:\")\n",
    "    print(\"  ✓ Simple API and good documentation\")\n",
    "    print(\"  ✓ Built-in persistence\")\n",
    "    print(\"  ⚠ Higher memory overhead\")\n",
    "    print(\"  ⚠ Slower for large datasets\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ChromaDB test failed: {e}\")\n",
    "    print(\"Continuing with alternative approaches...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Benchmarking RAG performance...\n",
      "\n",
      "📊 Performance Results:\n",
      "  Average time: 0.024s\n",
      "  Std deviation: 0.019s\n",
      "  Min time: 0.013s\n",
      "  Max time: 0.062s\n"
     ]
    }
   ],
   "source": [
    "# Performance Benchmarking\n",
    "import time\n",
    "\n",
    "def benchmark_rag(query, documents, model, iterations=5):\n",
    "    \"\"\"Benchmark RAG performance\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        start_time = time.time()\n",
    "        results = simple_rag(query, documents, model)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return {\n",
    "        'avg_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times)\n",
    "    }\n",
    "\n",
    "print(\"Benchmarking RAG performance...\")\n",
    "benchmark_results = benchmark_rag(\"What is machine learning?\", documents, model)\n",
    "\n",
    "print(f\"\\nPerformance Results:\")\n",
    "print(f\"  Average time: {benchmark_results['avg_time']:.3f}s\")\n",
    "print(f\"  Std deviation: {benchmark_results['std_time']:.3f}s\")\n",
    "print(f\"  Min time: {benchmark_results['min_time']:.3f}s\")\n",
    "print(f\"  Max time: {benchmark_results['max_time']:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 RAG Demo Complete!\n",
      "\n",
      "📋 What We Demonstrated:\n",
      "  ✅ Basic embedding generation\n",
      "  ✅ Similarity search\n",
      "  ✅ Simple RAG pipeline\n",
      "  ✅ ChromaDB integration (if available)\n",
      "  ✅ Performance benchmarking\n",
      "\n",
      "🔧 Why This Version Works:\n",
      "  ✅ No multiprocessing - avoids kernel crashes\n",
      "  ✅ Minimal memory usage - lightweight operation\n",
      "  ✅ Single-threaded - no complex parallel processing\n",
      "  ✅ Essential dependencies only\n",
      "\n",
      "🚀 Next Steps:\n",
      "  • Add more documents to the knowledge base\n",
      "  • Implement chunking strategies\n",
      "  • Add hybrid search (vector + keyword)\n",
      "  • Integrate with larger models\n",
      "  • Deploy to production environment\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"RAG Demo Complete!\")\n",
    "print(\"\\nWhat We Demonstrated:\")\n",
    "print(\"  • Basic embedding generation\")\n",
    "print(\"  • Similarity search\")\n",
    "print(\"  • RAG pipeline implementation\")\n",
    "print(\"  • Vector database integration\")\n",
    "print(\"  • Performance benchmarking\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  • Add more documents to the knowledge base\")\n",
    "print(\"  • Implement advanced chunking strategies\")\n",
    "print(\"  • Add hybrid search capabilities\")\n",
    "print(\"  • Integrate with larger language models\")\n",
    "print(\"  • Deploy to production environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced RAG with chunking defined\n"
     ]
    }
   ],
   "source": [
    "# Advanced RAG with Text Chunking\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"Text chunking for improved retrieval performance\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def enhanced_rag(query, documents, model, chunk_size=200, top_k=3):\n",
    "    \"\"\"Enhanced RAG with document chunking\"\"\"\n",
    "    # Chunk all documents\n",
    "    all_chunks = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        chunks = chunk_text(doc, chunk_size)\n",
    "        all_chunks.extend(chunks)\n",
    "        chunk_metadata.extend([{'doc_id': i, 'chunk_id': j} for j in range(len(chunks))])\n",
    "    \n",
    "    # Generate embeddings for all chunks\n",
    "    chunk_embeddings = model.encode(all_chunks)\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
    "    \n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': all_chunks[idx],\n",
    "            'similarity': similarities[idx],\n",
    "            'doc_id': chunk_metadata[idx]['doc_id'],\n",
    "            'chunk_id': chunk_metadata[idx]['chunk_id'],\n",
    "            'rank': len(results) + 1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Enhanced RAG with chunking defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing Enhanced RAG with Chunking...\n",
      "\n",
      "📊 Enhanced RAG Results:\n",
      "  📄 Rank 1: 0.664\n",
      "     Doc 0, Chunk 0\n",
      "     Content: Machine learning is transforming healthcare by enabling early disease detection ...\n",
      "\n",
      "  📄 Rank 2: 0.379\n",
      "     Doc 4, Chunk 0\n",
      "     Content: Deep learning neural networks mimic the human brain to solve complex problems in...\n",
      "\n",
      "  📄 Rank 3: 0.323\n",
      "     Doc 2, Chunk 0\n",
      "     Content: Natural language processing helps computers understand and generate human langua...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Enhanced RAG with Chunking\n",
    "print(\"Testing Enhanced RAG with Chunking...\")\n",
    "enhanced_results = enhanced_rag(\"How does machine learning help in healthcare?\", documents, model, chunk_size=100, top_k=3)\n",
    "\n",
    "print(f\"\\nEnhanced RAG Results:\")\n",
    "for result in enhanced_results:\n",
    "    print(f\"  Rank {result['rank']}: {result['similarity']:.3f}\")\n",
    "    print(f\"     Doc {result['doc_id']}, Chunk {result['chunk_id']}\")\n",
    "    print(f\"     Content: {result['chunk'][:80]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hybrid search function defined\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Search: Vector + Keyword Matching\n",
    "def hybrid_search(query, documents, model, vector_weight=0.7, keyword_weight=0.3, top_k=3):\n",
    "    \"\"\"Hybrid search combining vector similarity and keyword matching\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Vector similarity\n",
    "    query_embedding = model.encode([query])\n",
    "    doc_embeddings = model.encode(documents)\n",
    "    vector_scores = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Keyword matching\n",
    "    query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
    "    keyword_scores = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_words = set(re.findall(r'\\b\\w+\\b', doc.lower()))\n",
    "        if len(query_words) > 0:\n",
    "            overlap = len(query_words.intersection(doc_words))\n",
    "            keyword_score = overlap / len(query_words)\n",
    "        else:\n",
    "            keyword_score = 0\n",
    "        keyword_scores.append(keyword_score)\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores = (vector_weight * vector_scores + \n",
    "                    keyword_weight * np.array(keyword_scores))\n",
    "    \n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'document': documents[idx],\n",
    "            'vector_score': vector_scores[idx],\n",
    "            'keyword_score': keyword_scores[idx],\n",
    "            'hybrid_score': hybrid_scores[idx],\n",
    "            'rank': len(results) + 1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Hybrid search function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing Hybrid Search (Vector + Keyword)...\n",
      "\n",
      "📊 Hybrid Search Results:\n",
      "  📄 Rank 1: Hybrid Score 0.840\n",
      "     Vector: 0.771, Keyword: 1.000\n",
      "     Content: Machine learning is transforming healthcare by enabling earl...\n",
      "\n",
      "  📄 Rank 2: Hybrid Score 0.315\n",
      "     Vector: 0.307, Keyword: 0.333\n",
      "     Content: Deep learning neural networks mimic the human brain to solve...\n",
      "\n",
      "  📄 Rank 3: Hybrid Score 0.163\n",
      "     Vector: 0.232, Keyword: 0.000\n",
      "     Content: Computer vision systems can analyze images and videos to ide...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Hybrid Search\n",
    "print(\"Testing Hybrid Search (Vector + Keyword)...\")\n",
    "hybrid_results = hybrid_search(\"machine learning healthcare\", documents, model, top_k=3)\n",
    "\n",
    "print(f\"\\nHybrid Search Results:\")\n",
    "for result in hybrid_results:\n",
    "    print(f\"  Rank {result['rank']}: Hybrid Score {result['hybrid_score']:.3f}\")\n",
    "    print(f\"     Vector: {result['vector_score']:.3f}, Keyword: {result['keyword_score']:.3f}\")\n",
    "    print(f\"     Content: {result['document'][:60]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Interactive RAG Demo\n",
      "========================================\n",
      "📝 Sample queries you can try:\n",
      "  1. What is machine learning?\n",
      "  2. How does AI help in healthcare?\n",
      "  3. What are neural networks?\n",
      "  4. Tell me about computer vision\n",
      "  5. How do robots work?\n",
      "\n",
      "🔍 Testing all sample queries...\n",
      "\n",
      "--- Query 1: What is machine learning? ---\n",
      "Simple RAG:\n",
      "  0.489 - Machine learning is transforming healthcare by ena...\n",
      "  0.354 - Computer vision systems can analyze images and vid...\n",
      "Hybrid Search:\n",
      "  0.567 - Machine learning is transforming healthcare by ena...\n",
      "  0.314 - Deep learning neural networks mimic the human brai...\n",
      "\n",
      "--- Query 2: How does AI help in healthcare? ---\n",
      "Simple RAG:\n",
      "  0.456 - Machine learning is transforming healthcare by ena...\n",
      "  0.392 - Deep learning neural networks mimic the human brai...\n",
      "Hybrid Search:\n",
      "  0.369 - Machine learning is transforming healthcare by ena...\n",
      "  0.324 - Deep learning neural networks mimic the human brai...\n",
      "\n",
      "--- Query 3: What are neural networks? ---\n",
      "Simple RAG:\n",
      "  0.533 - Deep learning neural networks mimic the human brai...\n",
      "  0.258 - Computer vision systems can analyze images and vid...\n",
      "Hybrid Search:\n",
      "  0.523 - Deep learning neural networks mimic the human brai...\n",
      "  0.250 - Artificial intelligence and robotics are advancing...\n",
      "\n",
      "--- Query 4: Tell me about computer vision ---\n",
      "Simple RAG:\n",
      "  0.659 - Computer vision systems can analyze images and vid...\n",
      "  0.337 - Natural language processing helps computers unders...\n",
      "Hybrid Search:\n",
      "  0.581 - Computer vision systems can analyze images and vid...\n",
      "  0.236 - Natural language processing helps computers unders...\n",
      "\n",
      "--- Query 5: How do robots work? ---\n",
      "Simple RAG:\n",
      "  0.454 - Artificial intelligence and robotics are advancing...\n",
      "  0.314 - Computer vision systems can analyze images and vid...\n",
      "Hybrid Search:\n",
      "  0.318 - Artificial intelligence and robotics are advancing...\n",
      "  0.220 - Computer vision systems can analyze images and vid...\n"
     ]
    }
   ],
   "source": [
    "# Interactive RAG Testing\n",
    "def interactive_rag_demo():\n",
    "    \"\"\"Interactive RAG demo for testing different queries\"\"\"\n",
    "    print(\"Interactive RAG Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"What is machine learning?\",\n",
    "        \"How does AI help in healthcare?\", \n",
    "        \"What are neural networks?\",\n",
    "        \"Tell me about computer vision\",\n",
    "        \"How do robots work?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Sample queries:\")\n",
    "    for i, query in enumerate(sample_queries, 1):\n",
    "        print(f\"  {i}. {query}\")\n",
    "    \n",
    "    print(\"\\nTesting all sample queries...\")\n",
    "    \n",
    "    for i, query in enumerate(sample_queries, 1):\n",
    "        print(f\"\\n--- Query {i}: {query} ---\")\n",
    "        \n",
    "        # Test different methods\n",
    "        simple_results = simple_rag(query, documents, model, top_k=2)\n",
    "        hybrid_results = hybrid_search(query, documents, model, top_k=2)\n",
    "        \n",
    "        print(\"Simple RAG:\")\n",
    "        for result in simple_results:\n",
    "            print(f\"  {result['similarity']:.3f} - {result['document'][:50]}...\")\n",
    "        \n",
    "        print(\"Hybrid Search:\")\n",
    "        for result in hybrid_results:\n",
    "            print(f\"  {result['hybrid_score']:.3f} - {result['document'][:50]}...\")\n",
    "\n",
    "# Run interactive demo\n",
    "interactive_rag_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 RAG Performance Analysis\n",
      "========================================\n",
      "📈 Performance Results:\n",
      "Docs | Time (s) | Memory (MB)\n",
      "------------------------------\n",
      "   1 |   0.113 |      5.4\n",
      "   3 |   0.201 |     13.3\n",
      "   5 |   0.014 |     13.3\n",
      "  10 |   0.117 |     21.4\n",
      "  20 |   0.081 |     29.2\n",
      "\n",
      "🤖 Model Information:\n",
      "  Model: all-MiniLM-L6-v2\n",
      "  Dimension: 384\n",
      "  Memory usage: 29.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Performance and Memory Analysis\n",
    "def analyze_rag_performance():\n",
    "    \"\"\"Analyze RAG performance and memory usage\"\"\"\n",
    "    import psutil\n",
    "    import time\n",
    "    \n",
    "    print(\"RAG Performance Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Memory before\n",
    "    process = psutil.Process()\n",
    "    memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Test different document sizes\n",
    "    test_sizes = [1, 3, 5, 10, 20]\n",
    "    results = []\n",
    "    \n",
    "    for size in test_sizes:\n",
    "        test_docs = documents * (size // len(documents) + 1)\n",
    "        test_docs = test_docs[:size]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        test_results = simple_rag(\"What is machine learning?\", test_docs, model)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        results.append({\n",
    "            'doc_count': len(test_docs),\n",
    "            'time': end_time - start_time,\n",
    "            'memory_mb': memory_after - memory_before\n",
    "        })\n",
    "    \n",
    "    print(\"Performance Results:\")\n",
    "    print(\"Docs | Time (s) | Memory (MB)\")\n",
    "    print(\"-\" * 30)\n",
    "    for result in results:\n",
    "        print(f\"{result['doc_count']:4d} | {result['time']:7.3f} | {result['memory_mb']:8.1f}\")\n",
    "    \n",
    "    # Model info\n",
    "    print(f\"\\nModel Information:\")\n",
    "    print(f\"  Model: all-MiniLM-L6-v2\")\n",
    "    print(f\"  Dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    print(f\"  Memory usage: {memory_after - memory_before:.1f} MB\")\n",
    "\n",
    "analyze_rag_performance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Enhanced RAG Demo Complete!\n",
      "==================================================\n",
      "\n",
      "📋 What We Demonstrated:\n",
      "  ✅ Basic embedding generation\n",
      "  ✅ Simple similarity search\n",
      "  ✅ Enhanced RAG with chunking\n",
      "  ✅ Hybrid search (vector + keyword)\n",
      "  ✅ Interactive query testing\n",
      "  ✅ Performance analysis\n",
      "  ✅ ChromaDB integration (if available)\n",
      "\n",
      "🔧 Why This Version Works:\n",
      "  ✅ No multiprocessing - avoids kernel crashes\n",
      "  ✅ Minimal memory usage - lightweight operation\n",
      "  ✅ Single-threaded - no complex parallel processing\n",
      "  ✅ Essential dependencies only\n",
      "  ✅ Multiple search strategies\n",
      "  ✅ Performance monitoring\n",
      "\n",
      "🚀 Advanced Features Added:\n",
      "  • Text chunking for better retrieval\n",
      "  • Hybrid search combining vector + keyword matching\n",
      "  • Interactive query testing\n",
      "  • Performance and memory analysis\n",
      "  • Multiple search strategies comparison\n",
      "\n",
      "📈 Performance Characteristics:\n",
      "  • Fast embedding generation\n",
      "  • Low memory footprint\n",
      "  • Scalable to larger document sets\n",
      "  • Reliable on macOS\n",
      "\n",
      "🎯 Next Steps for Production:\n",
      "  • Add more sophisticated chunking strategies\n",
      "  • Implement query expansion\n",
      "  • Add result ranking and filtering\n",
      "  • Integrate with larger language models\n",
      "  • Deploy with proper vector databases\n",
      "  • Add caching for better performance\n",
      "\n",
      "💡 Key Takeaways:\n",
      "  • Simple approaches often work best\n",
      "  • Chunking improves retrieval quality\n",
      "  • Hybrid search combines best of both worlds\n",
      "  • Performance monitoring is essential\n",
      "  • Lightweight solutions are more reliable\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"RAG Demo Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nWhat We Demonstrated:\")\n",
    "print(\"  • Basic embedding generation\")\n",
    "print(\"  • Similarity search\")\n",
    "print(\"  • RAG with document chunking\")\n",
    "print(\"  • Hybrid search (vector + keyword)\")\n",
    "print(\"  • Interactive query testing\")\n",
    "print(\"  • Performance analysis\")\n",
    "print(\"  • Vector database integration\")\n",
    "\n",
    "print(\"\\nAdvanced Features:\")\n",
    "print(\"  • Text chunking for better retrieval\")\n",
    "print(\"  • Hybrid search combining vector + keyword matching\")\n",
    "print(\"  • Interactive query testing\")\n",
    "print(\"  • Performance and memory analysis\")\n",
    "print(\"  • Multiple search strategies comparison\")\n",
    "\n",
    "print(\"\\nPerformance Characteristics:\")\n",
    "print(\"  • Fast embedding generation\")\n",
    "print(\"  • Low memory footprint\")\n",
    "print(\"  • Scalable to larger document sets\")\n",
    "print(\"  • Reliable operation\")\n",
    "\n",
    "print(\"\\nNext Steps for Production:\")\n",
    "print(\"  • Add more sophisticated chunking strategies\")\n",
    "print(\"  • Implement query expansion\")\n",
    "print(\"  • Add result ranking and filtering\")\n",
    "print(\"  • Integrate with larger language models\")\n",
    "print(\"  • Deploy with proper vector databases\")\n",
    "print(\"  • Add caching for better performance\")\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"  • Simple approaches often work best\")\n",
    "print(\"  • Chunking improves retrieval quality\")\n",
    "print(\"  • Hybrid search combines best of both worlds\")\n",
    "print(\"  • Performance monitoring is essential\")\n",
    "print(\"  • Lightweight solutions are more reliable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
