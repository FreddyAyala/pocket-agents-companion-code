# Chapter 7: Fine-Tuning & Adaptation
# On-Device AI: The Small Language Models Revolution

# Core dependencies
torch>=2.0.0
transformers>=4.30.0
datasets>=2.10.0
accelerate>=0.20.0
peft>=0.5.0  # For LoRA, QLoRA, and other PEFT methods
bitsandbytes>=0.40.0  # For quantization during training
scipy>=1.10.0  # For bitsandbytes

# Training utilities
wandb>=0.15.0  # For experiment tracking (optional)
tensorboard>=2.10.0  # For training visualization
evaluate>=0.4.0  # For model evaluation

# Data processing
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.5.0

# Jupyter notebook support
jupyter>=1.0.0
ipykernel>=6.0.0