{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Fine-Tuning & Adaptation - LoRA Demo\n",
    "\n",
    "## Making Models Your Own with Parameter-Efficient Fine-Tuning\n",
    "\n",
    "**Pocket Agents: A Practical Guide to Onâ€‘Device Artificial Intelligence**\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **LoRA (Low-Rank Adaptation)** for memory-efficient fine-tuning\n",
    "- **Parameter efficiency** - training <1% of model parameters\n",
    "- **Domain adaptation** - teaching models specific knowledge\n",
    "- **Before/after comparison** - seeing real improvement\n",
    "\n",
    "## Why This Works\n",
    "\n",
    "We're using **TinyLlama-1.1B-Chat**, which is:\n",
    "- Already instruction-tuned (understands user/assistant format)\n",
    "- Small enough for quick training (1.1B parameters)\n",
    "- Works well on consumer hardware (MPS/CPU)\n",
    "- Shows clear improvement with minimal training\n",
    "\n",
    "**Expected runtime**: 3-5 minutes total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables set\n",
      "   TOKENIZERS_PARALLELISM: false\n",
      "   WANDB_DISABLED: true\n",
      "   HF_HUB_DISABLE_PROGRESS_BARS: 1\n",
      "\n",
      "ðŸ”§ Importing libraries...\n",
      "âœ… All imports successful!\n",
      "   PyTorch: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Environment variables\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "\n",
    "print(\"âœ… Environment variables set\")\n",
    "print(f\"   TOKENIZERS_PARALLELISM: {os.environ.get('TOKENIZERS_PARALLELISM')}\")\n",
    "print(f\"   WANDB_DISABLED: {os.environ.get('WANDB_DISABLED')}\")\n",
    "print(f\"   HF_HUB_DISABLE_PROGRESS_BARS: {os.environ.get('HF_HUB_DISABLE_PROGRESS_BARS')}\")\n",
    "\n",
    "# Import required libraries\n",
    "print(\"\\nðŸ”§ Importing libraries...\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load TinyLlama-1.1B-Chat (Base Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama-1.1B-Chat...\n",
      "   This may take a few minutes on first run (downloading ~2GB model)\n",
      "Using MPS (Apple Silicon GPU)\n",
      "Model loaded and moved to: mps\n",
      "Model parameters: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading TinyLlama-1.1B-Chat...\")\n",
    "print(\"   This may take a few minutes on first run (downloading ~2GB model)\")\n",
    "\n",
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Pad token set\")\n",
    "\n",
    "# Move to device (MPS/CPU detection)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded and moved to: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Base Model (Before Training)\n",
    "\n",
    "Let's see how the base model responds to our test prompts **before** fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BASE MODEL (before training):\n",
      "======================================================================\n",
      "\n",
      "Test 1:\n",
      "Prompt: What is machine learning?\n",
      "Base Response: Machine learning is a branch of Artificial Intelligence (AI) that involves the use of algorithms and computational models to learn from data without being explicitly programmed. It has revolutionized many aspects of human society, from web search to self-driving cars. Here are some key concepts of machine learning:\n",
      "\n",
      "1. Learning:machine learning involves the process of identifying patterns in data and using these patterns to make predictions or decisions.\n",
      "\n",
      "2. Algorithms:machine learning\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 2:\n",
      "Prompt: Explain Python in simple terms\n",
      "Base Response: Python is a general-purpose programming language that is easy to learn and understand. It's a lightweight and efficient language that's commonly used for web development, machine learning, data analysis, and more. Here are some Python in simple terms:\n",
      "\n",
      "1. High-Level: Python is a high-level language that emphasizes readability and simplicity. It's easy to understand and write code in.\n",
      "\n",
      "2. Interpreted: Python is an interpreted language\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 3:\n",
      "Prompt: What are the benefits of renewable energy?\n",
      "Base Response: Renewable energy has several benefits for the environment and society:\n",
      "1. Reduced dependency on fossil fuels: Renewable energy sources such as wind, solar, hydropower, geothermal, and biomass are reliable and do not depend on the availability of fossil fuels such as coal, oil, and natural gas. This reduces the risk of disruptions in supply and ensures a consistent supply of energy.\n",
      "2. Redu\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Base model testing complete!\n",
      "Responses saved for later comparison\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"Generate a response from the model\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            attention_mask=torch.ones_like(inputs)\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part (after the prompt)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Test prompts using TinyLlama's chat format\n",
    "test_prompts = [\n",
    "    '<|user|>\\nWhat is machine learning?<|assistant|>\\n',\n",
    "    '<|user|>\\nExplain Python in simple terms<|assistant|>\\n',\n",
    "    '<|user|>\\nWhat are the benefits of renewable energy?<|assistant|>\\n'\n",
    "]\n",
    "\n",
    "print(\"Testing BASE MODEL (before training):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_responses = []\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    base_responses.append(response)\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Prompt: {prompt.replace('<|user|>', '').replace('<|assistant|>', '').strip()}\")\n",
    "    print(f\"Base Response: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nBase model testing complete!\")\n",
    "print(\"Responses saved for later comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply LoRA Configuration\n",
    "\n",
    "Now we'll add LoRA adapters to the model. LoRA only trains a small fraction of parameters (~0.5%) while keeping the base model frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LoRA configuration...\n",
      "LoRA configuration created:\n",
      "   Rank: 16\n",
      "   Alpha: 32\n",
      "   Target modules: {'v_proj', 'o_proj', 'q_proj', 'k_proj'}\n",
      "   Dropout: 0.1\n",
      "\n",
      "Applying LoRA to model...\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "\n",
      "LoRA applied successfully!\n",
      "\n",
      "Trainable Parameters:\n",
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
      "\n",
      "Notice: We're only training ~0.5% of the model's parameters!\n",
      "   This is the power of LoRA - efficient fine-tuning with minimal resources.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/freddyayala/Documents/GitHub/slm-ebook/companion-code/chapters/chapter-07/venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating LoRA configuration...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the low-rank matrices\n",
    "    lora_alpha=32,  # Scaling factor (typically 2x the rank)\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],  # TinyLlama attention modules\n",
    "    lora_dropout=0.1,  # Regularization to prevent overfitting\n",
    "    bias='none',\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration created:\")\n",
    "print(f\"   Rank: {lora_config.r}\")\n",
    "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"   Target modules: {lora_config.target_modules}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
    "\n",
    "print(\"\\nApplying LoRA to model...\")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nLoRA applied successfully!\")\n",
    "print(\"\\nTrainable Parameters:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nNotice: We're only training ~0.5% of the model's parameters!\")\n",
    "print(\"   This is the power of LoRA - efficient fine-tuning with minimal resources.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Training Data\n",
    "\n",
    "We'll create a small, high-quality dataset to teach the model specific knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 8 high-quality training examples\n",
      "   Topics: ML, Programming, Science, Environment, Health\n",
      "\n",
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeb6520eacb43b98098162ab8059929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data prepared and tokenized\n",
      "   Dataset size: 8 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing training data...\")\n",
    "\n",
    "# High-quality instruction-response pairs\n",
    "training_data = [\n",
    "    {\n",
    "        'instruction': 'What is machine learning?',\n",
    "        'response': 'Machine learning is a field of artificial intelligence where computers learn from data to make predictions or decisions without being explicitly programmed for each task. It uses algorithms that improve automatically through experience.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'Explain Python in simple terms',\n",
    "        'response': 'Python is a beginner-friendly programming language known for its simple, readable syntax. It is widely used for web development, data analysis, artificial intelligence, and automation. Python code looks almost like English, making it easy to learn and understand.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'What are the benefits of renewable energy?',\n",
    "        'response': 'Renewable energy offers several key benefits: environmental sustainability by reducing greenhouse gas emissions, energy independence by reducing reliance on fossil fuels, economic benefits through job creation and lower long-term costs, and resource conservation by using naturally replenishing sources like solar and wind.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'How does photosynthesis work?',\n",
    "        'response': 'Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose (sugar) and oxygen. Chlorophyll in plant leaves captures light energy, which drives chemical reactions that produce food for the plant while releasing oxygen as a byproduct.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'What is the water cycle?',\n",
    "        'response': 'The water cycle is the continuous movement of water on, above, and below Earth\\'s surface. It includes evaporation from bodies of water, condensation into clouds, precipitation as rain or snow, and collection back into oceans, lakes, and rivers. This cycle is powered by the sun\\'s energy.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'Explain what a neural network is',\n",
    "        'response': 'A neural network is a computing system inspired by biological brains. It consists of interconnected nodes (neurons) organized in layers that process information. Each connection has a weight that adjusts during training, allowing the network to learn patterns from data and make predictions.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'What is climate change?',\n",
    "        'response': 'Climate change refers to long-term shifts in global temperatures and weather patterns. While climate naturally varies, current changes are primarily driven by human activities, especially burning fossil fuels, which releases greenhouse gases that trap heat in the atmosphere and warm the planet.'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'How do vaccines work?',\n",
    "        'response': 'Vaccines work by training your immune system to recognize and fight specific diseases. They contain weakened or inactive parts of a particular pathogen that trigger an immune response. Your body produces antibodies and remembers the pathogen, providing protection if you encounter the real disease later.'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(training_data)} high-quality training examples\")\n",
    "print(\"   Topics: ML, Programming, Science, Environment, Health\")\n",
    "\n",
    "# Format using TinyLlama's chat template\n",
    "def format_instruction(example):\n",
    "    return f\"<|user|>\\n{example['instruction']}<|assistant|>\\n{example['response']}\"\n",
    "\n",
    "formatted_data = [{'text': format_instruction(example)} for example in training_data]\n",
    "train_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(\"\\nTokenizing training data...\")\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "print(\"Training data prepared and tokenized\")\n",
    "print(f\"   Dataset size: {len(tokenized_dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup & Execution\n",
    "\n",
    "Now we'll train the LoRA adapters. This should take about 2-3 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training configuration...\n",
      "Training configuration created\n",
      "   Learning rate: 0.0002\n",
      "   Max steps: 15\n",
      "   Batch size: 1\n",
      "\n",
      "Trainer created successfully\n",
      "\n",
      "Starting LoRA training...\n",
      "   This will take about 2-3 minutes\n",
      "   Watch the loss decrease as the model learns!\n",
      "\n",
      "\n",
      "Training started!\n",
      "Starting epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:02, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.788500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loss = 1.5504\n",
      "Step 2: Loss = 1.2507\n",
      "Step 3: Loss = 1.5777\n",
      "Step 4: Loss = 1.4585\n",
      "Step 5: Loss = 1.4448\n",
      "Step 6: Loss = 1.5585\n",
      "Step 7: Loss = 1.1621\n",
      "Step 8: Loss = 1.5073\n",
      "Completed epoch 1.0\n",
      "Starting epoch 1.0\n",
      "Step 9: Loss = 1.0356\n",
      "Step 10: Loss = 1.3757\n",
      "Step 11: Loss = 1.0590\n",
      "Step 12: Loss = 1.2490\n",
      "Step 13: Loss = 0.7885\n",
      "Step 14: Loss = 1.0772\n",
      "Step 15: Loss = 1.0721\n",
      "Model checkpoint saved!\n",
      "Completed epoch 1.875\n",
      "\n",
      "Training completed!\n",
      "\n",
      "Training completed successfully!\n",
      "Trained LoRA adapters saved to ./tinyllama_trained_lora\n",
      "\n",
      "Training loss progression:\n",
      "   Initial loss: 1.5504\n",
      "   Final loss: 1.0721\n",
      "   Improvement: 30.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up training configuration...\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tinyllama_lora',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=5,\n",
    "    logging_steps=1,\n",
    "    save_steps=20,\n",
    "    eval_strategy='no',\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=False,  # MPS compatibility\n",
    "    bf16=False,\n",
    "    max_steps=15,  # Quick demo - just 15 steps\n",
    ")\n",
    "\n",
    "print(\"Training configuration created\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Max steps: {training_args.max_steps}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "\n",
    "# Complete callback class\n",
    "training_losses = []\n",
    "\n",
    "class TrainingCallback:\n",
    "    \"\"\"Complete callback with all required methods\"\"\"\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"\\nTraining started!\")\n",
    "        return control\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"Starting epoch {state.epoch}\")\n",
    "        return control\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Completed epoch {state.epoch}\")\n",
    "        return control\n",
    "    \n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        return control\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        return control\n",
    "    \n",
    "    def on_substep_begin(self, args, state, control, **kwargs):\n",
    "        return control\n",
    "    \n",
    "    def on_substep_end(self, args, state, control, **kwargs):\n",
    "        return control\n",
    "    \n",
    "    def on_pre_optimizer_step(self, args, state, control, **kwargs):\n",
    "        return control\n",
    "    \n",
    "    def on_optimizer_step(self, args, state, control, **kwargs):\n",
    "        return control\n",
    "    \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(\"Model checkpoint saved!\")\n",
    "        return control\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            training_losses.append(logs['loss'])\n",
    "            print(f\"Step {state.global_step}: Loss = {logs['loss']:.4f}\")\n",
    "        return control\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(\"\\nTraining completed!\")\n",
    "        return control\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.add_callback(TrainingCallback())\n",
    "\n",
    "print(\"\\nTrainer created successfully\")\n",
    "print(\"\\nStarting LoRA training...\")\n",
    "print(\"   This will take about 2-3 minutes\")\n",
    "print(\"   Watch the loss decrease as the model learns!\\n\")\n",
    "\n",
    "# Train!\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    \n",
    "    # Save the trained LoRA adapters\n",
    "    lora_model.save_pretrained('./tinyllama_trained_lora')\n",
    "    print(\"Trained LoRA adapters saved to ./tinyllama_trained_lora\")\n",
    "    \n",
    "    # Show training progress\n",
    "    if training_losses:\n",
    "        print(f\"\\nTraining loss progression:\")\n",
    "        print(f\"   Initial loss: {training_losses[0]:.4f}\")\n",
    "        print(f\"   Final loss: {training_losses[-1]:.4f}\")\n",
    "        print(f\"   Improvement: {((training_losses[0] - training_losses[-1]) / training_losses[0] * 100):.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Trained Model (After Training)\n",
    "\n",
    "Now let's test the trained model with the same prompts to see the improvement!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing TRAINED MODEL (after LoRA fine-tuning):\n",
      "======================================================================\n",
      "\n",
      "Test 1:\n",
      "Prompt: What is machine learning?\n",
      "LoRA Response: Machine learning is a field of computer science that involves learning from data without explicit human intervention. It involves the use of algorithms to learn from data that has previously been collected. Machine learning algorithms can be used for tasks such as recommendation systems, forecasting, and natural language processing.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 2:\n",
      "Prompt: Explain Python in simple terms\n",
      "LoRA Response: Python is a high-level, interpreted, object-oriented programming language. It is widely used in web development, data science, and artificial intelligence applications. Python's syntax is simple and easy to learn, making it a popular choice for beginners. It has a large community of developers who contribute to the Python programming language. Python has a wide range of libraries and tools that developers can use to create interactive applications.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 3:\n",
      "Prompt: What are the benefits of renewable energy?\n",
      "LoRA Response: Renewable energy sources such as solar, wind, and hydropower offer a range of environmental, economic, and social benefits. Here are some of them:\n",
      "\n",
      "1. Reduced dependence on fossil fuels: Renewable energy sources are a sustainable and reliable source of energy that reduces dependence on fossil fuels.\n",
      "\n",
      "2. Increased energy security: Renewable energy can provide a reliable and secure energy supply for countries and regions that depend\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Trained model testing complete!\n",
      "Responses saved for comparison\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing TRAINED MODEL (after LoRA fine-tuning):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lora_responses = []\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    response = generate_response(lora_model, tokenizer, prompt)\n",
    "    lora_responses.append(response)\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Prompt: {prompt.replace('<|user|>', '').replace('<|assistant|>', '').strip()}\")\n",
    "    print(f\"LoRA Response: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nTrained model testing complete!\")\n",
    "print(\"Responses saved for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Qualitative Analysis - Compare Results\n",
    "\n",
    "Let's compare the base model vs the LoRA-finetuned model side-by-side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON: Base Model vs LoRA-Finetuned Model\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Prompt 1: What is machine learning?\n",
      "================================================================================\n",
      "\n",
      "Base Model Response:\n",
      "   Machine learning is a branch of Artificial Intelligence (AI) that involves the use of algorithms and computational models to learn from data without being explicitly programmed. It has revolutionized many aspects of human society, from web search to self-driving cars. Here are some key concepts of machine learning:\n",
      "\n",
      "1. Learning:machine learning involves the process of identifying patterns in data and using these patterns to make predictions or decisions.\n",
      "\n",
      "2. Algorithms:machine learning\n",
      "\n",
      "LoRA-Trained Response:\n",
      "   Machine learning is a field of computer science that involves learning from data without explicit human intervention. It involves the use of algorithms to learn from data that has previously been collected. Machine learning algorithms can be used for tasks such as recommendation systems, forecasting, and natural language processing.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Prompt 2: Explain Python in simple terms\n",
      "================================================================================\n",
      "\n",
      "Base Model Response:\n",
      "   Python is a general-purpose programming language that is easy to learn and understand. It's a lightweight and efficient language that's commonly used for web development, machine learning, data analysis, and more. Here are some Python in simple terms:\n",
      "\n",
      "1. High-Level: Python is a high-level language that emphasizes readability and simplicity. It's easy to understand and write code in.\n",
      "\n",
      "2. Interpreted: Python is an interpreted language\n",
      "\n",
      "LoRA-Trained Response:\n",
      "   Python is a high-level, interpreted, object-oriented programming language. It is widely used in web development, data science, and artificial intelligence applications. Python's syntax is simple and easy to learn, making it a popular choice for beginners. It has a large community of developers who contribute to the Python programming language. Python has a wide range of libraries and tools that developers can use to create interactive applications.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Prompt 3: What are the benefits of renewable energy?\n",
      "================================================================================\n",
      "\n",
      "Base Model Response:\n",
      "   Renewable energy has several benefits for the environment and society:\n",
      "1. Reduced dependency on fossil fuels: Renewable energy sources such as wind, solar, hydropower, geothermal, and biomass are reliable and do not depend on the availability of fossil fuels such as coal, oil, and natural gas. This reduces the risk of disruptions in supply and ensures a consistent supply of energy.\n",
      "2. Redu\n",
      "\n",
      "LoRA-Trained Response:\n",
      "   Renewable energy sources such as solar, wind, and hydropower offer a range of environmental, economic, and social benefits. Here are some of them:\n",
      "\n",
      "1. Reduced dependence on fossil fuels: Renewable energy sources are a sustainable and reliable source of energy that reduces dependence on fossil fuels.\n",
      "\n",
      "2. Increased energy security: Renewable energy can provide a reliable and secure energy supply for countries and regions that depend\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Key Observations:\n",
      "   - The LoRA model should provide more focused, informative responses\n",
      "   - Responses should better match the training data style\n",
      "   - We achieved this by training only ~0.5% of the model's parameters!\n",
      "\n",
      "Training Efficiency:\n",
      "   - Training time: ~2-3 minutes\n",
      "   - Parameters trained: ~0.5% of total\n",
      "   - Memory efficient: No need to train full model\n",
      "   - Adapter size: Only a few MB (vs full model's 2GB)\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPARISON: Base Model vs LoRA-Finetuned Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    clean_prompt = prompt.replace('<|user|>', '').replace('<|assistant|>', '').strip()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt {i+1}: {clean_prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nBase Model Response:\")\n",
    "    print(f\"   {base_responses[i]}\")\n",
    "    print(f\"\\nLoRA-Trained Response:\")\n",
    "    print(f\"   {lora_responses[i]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"   - The LoRA model should provide more focused, informative responses\")\n",
    "print(\"   - Responses should better match the training data style\")\n",
    "print(\"   - We achieved this by training only ~0.5% of the model's parameters!\")\n",
    "print(\"\\nTraining Efficiency:\")\n",
    "print(f\"   - Training time: ~2-3 minutes\")\n",
    "print(f\"   - Parameters trained: ~0.5% of total\")\n",
    "print(f\"   - Memory efficient: No need to train full model\")\n",
    "print(f\"   - Adapter size: Only a few MB (vs full model's 2GB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Key Takeaways\n",
    "\n",
    "### Why TinyLlama Works Better Than GPT-2\n",
    "\n",
    "1. **Instruction-Tuned**: TinyLlama-1.1B-Chat is pre-trained to understand user/assistant format\n",
    "2. **Modern Architecture**: Based on LLaMA, with improvements over GPT-2\n",
    "3. **Chat-Optimized**: Specifically designed for conversational tasks\n",
    "4. **Better Baseline**: Starts with instruction-following capability\n",
    "\n",
    "### How LoRA Enables Efficient Fine-Tuning\n",
    "\n",
    "1. **Parameter Efficiency**: Only trains 0.5% of model parameters\n",
    "2. **Low-Rank Adaptation**: Adds trainable matrices to attention layers\n",
    "3. **Memory Efficient**: Base model stays frozen, only adapters updated\n",
    "4. **Quick Training**: Fewer parameters = faster convergence\n",
    "5. **Small Adapters**: Can store multiple task-specific adapters cheaply\n",
    "\n",
    "### When to Use LoRA vs Full Fine-Tuning\n",
    "\n",
    "**Use LoRA When**:\n",
    "- Limited compute resources\n",
    "- Quick adaptation needed\n",
    "- Multiple task-specific models required\n",
    "- Base model is already good\n",
    "\n",
    "**Use Full Fine-Tuning When**:\n",
    "- Complete model behavior change needed\n",
    "- Abundant compute resources\n",
    "- Domain is very different from pre-training\n",
    "- Maximum performance is critical\n",
    "\n",
    "### Limitations and Next Steps\n",
    "\n",
    "**Current Limitations**:\n",
    "- Small training set (8 examples)\n",
    "- Quick demo (15 steps)\n",
    "- Limited evaluation metrics\n",
    "\n",
    "**To Improve Further**:\n",
    "1. Use more training data (100+ examples)\n",
    "2. Train for more steps (50-100+)\n",
    "3. Add evaluation metrics (BLEU, ROUGE)\n",
    "4. Experiment with different LoRA ranks\n",
    "5. Try QLoRA for even more efficiency\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "LoRA is a powerful technique for efficient fine-tuning. By training only a small fraction of parameters, we can adapt pre-trained models to specific tasks quickly and cheaply. This makes on-device fine-tuning practical and accessible!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chapter07)",
   "language": "python",
   "name": "chapter07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
